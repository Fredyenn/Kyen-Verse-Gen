{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rap Lyrics Generator - Kanye West\n",
    "\n",
    "## Introduction\n",
    "This project uses 300+ Kanye West's verses to train on Recurrent Nural Network (RNN). The verses file is [from a dataset on Kaggle](https://www.kaggle.com/viccalexander/kanyewestverses). The dataset contains total of 364 verses from 243 songs. Each verses are seprated by an empty line which consist of \"\\n\". This project is trained on characters based features. i.e. the embeding step is based on characters, not words.\n",
    "This project is inspried by a NLP course on coursera in Tensorflow in practice specitialization offer by deeplearning.ai and [Tensorflow tutorial](https://www.tensorflow.org/tutorials/text/text_generation). The process is to prepare the lyrics and window them into segments the use the characters in these windows to predict the next character. Since each character is related to its previous and next character, RNN and LSTM that carry hidden state from previous cell are suitable for this application. \n",
    "\n",
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2840,
     "status": "ok",
     "timestamp": 1574708262548,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "p-BI8gkj4Hp1",
    "outputId": "41d1274b-cd51-44ee-c4af-d81bd53a67c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "#!pip install tensorflow==2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the file print the first 250 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2770,
     "status": "ok",
     "timestamp": 1574708262552,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "RgeWVKN94HqN",
    "outputId": "4eab332c-1b01-4e9b-bb3f-d9ea7befc86b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 260341 characters\n",
      "Let the suicide doors up\n",
      "I threw suicides on the tour bus\n",
      "I threw suicides on the private jet\n",
      "You know what that mean, I'm fly to death\n",
      "I step in Def Jam buildin' like I'm the shit\n",
      "Tell 'em give me fifty million or I'ma quit\n",
      "Most rappers' taste level\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open('kanye_verses.txt', 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "\n",
    "print ('Length of text: {} characters'.format(len(text)))\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique character\n",
    "We are using character based feature, so we want to identify the total number of unique charactor and give each one of them a numerical label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2670,
     "status": "ok",
     "timestamp": 1574708262555,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "8HzRqvHK4Hqd",
    "outputId": "fd5d512a-3cc1-48b5-d5b5-e421b25f0fc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 unique characters:\n",
      "['\\n', ' ', '!', '\"', '#', '$', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~', '·', 'Á', 'é', 'í', 'ñ', 'ó', 'ā', '\\u200b', '–', '‘', '’', '“', '”', '…']\n",
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '&': 6, \"'\": 7, '(': 8, ')': 9, '*': 10, '+': 11, ',': 12, '-': 13, '.': 14, '/': 15, '0': 16, '1': 17, '2': 18, '3': 19, '4': 20, '5': 21, '6': 22, '7': 23, '8': 24, '9': 25, ':': 26, ';': 27, '?': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, 'a': 55, 'b': 56, 'c': 57, 'd': 58, 'e': 59, 'f': 60, 'g': 61, 'h': 62, 'i': 63, 'j': 64, 'k': 65, 'l': 66, 'm': 67, 'n': 68, 'o': 69, 'p': 70, 'q': 71, 'r': 72, 's': 73, 't': 74, 'u': 75, 'v': 76, 'w': 77, 'x': 78, 'y': 79, 'z': 80, '~': 81, '·': 82, 'Á': 83, 'é': 84, 'í': 85, 'ñ': 86, 'ó': 87, 'ā': 88, '\\u200b': 89, '–': 90, '‘': 91, '’': 92, '“': 93, '”': 94, '…': 95}\n"
     ]
    }
   ],
   "source": [
    "# number of unique characters \n",
    "print(len(set(text)),'unique characters:') \n",
    "\n",
    "# dict of these characters\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "\n",
    "char_dict = {char:i for i,char in enumerate(chars)}\n",
    "idx2char = np.array(chars)\n",
    "print(char_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform lyrics to numerical labels\n",
    "Here each characters are transform to relative numerical label that I created above so they can be then feed into Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2552,
     "status": "ok",
     "timestamp": 1574708262558,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "V78sCket4Hqr",
    "outputId": "f2b83a6f-b0fb-4670-9f3f-937a11d8b0de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let the suici\n",
      "[40 59 74  1 74 62 59  1 73 75 63 57 63]\n",
      "'Let the suici' ---- characters mapped to int ---- > [40 59 74  1 74 62 59  1 73 75 63 57 63]\n"
     ]
    }
   ],
   "source": [
    "#Sequence of the text\n",
    "text_in_num =np.array([char_dict[i] for i in text])\n",
    "print(text[:13])\n",
    "print(text_in_num[:13])\n",
    "\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_in_num[:13]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequences for training X and Y\n",
    "In order to create X (feature set) and Y, the answer,(ground truth), the entire lyrics file are flaten to a single vactor(1,# of chars). The sequences (training samples) are extracted by shifting a window down to the vector. For example, If the lyrics file in a single vector is : \\[0,1,2,3,4,5,6,7,8,9\\]. We use a window size of 3 and shift every 1 character, we will get:\\[0,1,2\\] \\[1,2,3\\] \\[2,3,4\\] \\[3,4,5\\] \\[4,5,6\\] \\[5,6,7\\] \\[6,7,8\\] and \\[7,8,9\\].\n",
    "\n",
    "The window size is defined by the length of each sequence(training sample). The sample is the first character to n-1 th. The second to the last characters are our target, Y. i.e. an instance is \\[0,1,2,3\\]: Training sequence :\\[0,1,2\\], target:\\[1,2,3\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2650,
     "status": "ok",
     "timestamp": 1574708262748,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "J35F4NZx4Hq2",
    "outputId": "3b8869d7-d3a9-47db-d156-7c5935cc9da7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52058"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert to trainable data\n",
    "seq_len = 50\n",
    "example_per_epoc = (len(text_in_num)-seq_len)//5\n",
    "BATCH_SIZE = 64\n",
    "example_per_epoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2596,
     "status": "ok",
     "timestamp": 1574708262751,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "JPSeMcEG4HrI",
    "outputId": "1da38800-f716-4f85-a76c-929f327d2e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:(batch_size, sequence_len)\n",
      "(64, 50)\n",
      "Target:(batch_size,)\n",
      "(64, 50)\n"
     ]
    }
   ],
   "source": [
    "# Create training examples / targets\n",
    "\n",
    "#Make the data in numerical lable to Tf.dataset for furter processing\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_in_num)\n",
    "\n",
    "#Creating sequences \n",
    "sequences = char_dataset.window(size = seq_len+1, shift = 10, drop_remainder = True)\n",
    "\n",
    "#Get training instance (training sequence + Target) and Flaten out the entire dataset \n",
    "sequences = sequences.flat_map(lambda window: window.batch(seq_len+1))\n",
    "\n",
    "#Creating tuple for each train instance : (sequence, target)\n",
    "dataset = sequences.map(lambda window: (window[:-1], window[1:]))\n",
    "\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "\n",
    "for i,y in dataset.take(1):\n",
    "    print('Sample:(batch_size, sequence_len)')\n",
    "    print(i.numpy().shape)\n",
    "    print('Target:(batch_size,)')\n",
    "    print(y.numpy().shape)\n",
    "    \n",
    "#Sample:(batch_size, sequence_len)\n",
    "#(64, 50)\n",
    "#Target:(batch_size,)\n",
    "#(64, 50)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the first 10 sequences with window size 50 and shift = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2559,
     "status": "ok",
     "timestamp": 1574708262754,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "sGAW7OOL4HrT",
    "outputId": "bcc4177f-ce60-4ad8-b1cb-f64ef0be27df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"le get money, don't spend it\\nOr maybe they get mon\"\n",
      "Input data:  'ld you\\nBefore the blood on the leaves\\nI know there'\n",
      "Input data:  \"e long-ass verses\\nI'm tired, you tired, Jesus wept\"\n",
      "Input data:  'nly felony\\nSo never fuck nobody without telling me'\n",
      "Input data:  'itchAlert\\nHe Instagram his watch like #MadRichAler'\n",
      "Input data:  'o you bring home\\nThey see a black man with a white'\n",
      "Input data:  'out the girl in all leopard\\nWho was rubbing the wo'\n",
      "Input data:  \"n one day\\nEverytime I'm in Vegas they screaming li\"\n",
      "Input data:  'ething out me\\nThen they talk about me\\nWould be los'\n",
      "Input data:  'ival - the un-American idols\\nWhat niggas did in Pa'\n"
     ]
    }
   ],
   "source": [
    "for batch,batch_target in  dataset.take(1):\n",
    "    for i in range(10):\n",
    "        print ('Input data: ', repr(''.join(idx2char[batch[i].numpy()])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjizZxG44Hre"
   },
   "source": [
    "### The first 5 sequences and respective targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2466,
     "status": "ok",
     "timestamp": 1574708262913,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "ynOc_FYQ4Hrm",
    "outputId": "1d29b4ed-c6f8-466a-b22c-dd1885aac412",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"I'd be worried if they said nothing\\nRemind me wher\"\n",
      "Target data: \"'d be worried if they said nothing\\nRemind me where\"\n",
      "Input data:  \"st step\\nAnd hey, you know ain't nobody perfect\\nAnd\"\n",
      "Target data: \"t step\\nAnd hey, you know ain't nobody perfect\\nAnd \"\n",
      "Input data:  'that alimony, uh, yeah-yeah, she got you homie, ye'\n",
      "Target data: 'hat alimony, uh, yeah-yeah, she got you homie, yea'\n",
      "Input data:  \"now\\nIt's been racin' since the summertime\\nNow I'm \"\n",
      "Target data: \"ow\\nIt's been racin' since the summertime\\nNow I'm h\"\n",
      "Input data:  'rst molly\\nAnd came out of your body\\nAnd came out o'\n",
      "Target data: 'st molly\\nAnd came out of your body\\nAnd came out of'\n"
     ]
    }
   ],
   "source": [
    "for batch,batch_target in  dataset.take(1):\n",
    "    for i in range(5):\n",
    "        print ('Input data: ', repr(''.join(idx2char[batch[i].numpy()])))\n",
    "        print ('Target data:', repr(''.join(idx2char[batch_target[i].numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2432,
     "status": "ok",
     "timestamp": 1574708262917,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "rVZsMEEd4Hr9",
    "outputId": "a8e8b2fe-df84-4e54-c7fd-763b889b8bd7"
   },
   "source": [
    "### Build the model\n",
    "The model consists of an embeding layer, a RNN (GRU) layer, and a dense output layer. As the \"return_sequence\" is set to True, the output of GRU layer is output the state of each call in the GRU. The dense lyaer outputs the probability of each charater for each state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fy4n3BOo4HsJ"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dTY2zdCW4HsY"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    # Emdedding layer    \n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),  \n",
    "    # RNN layer    \n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "        \n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True),\n",
    "    # Dense layer    \n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "      ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "  vocab_size = len(chars),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look what each batch looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[37 74  7 ...  1 56 55]\n",
      " [59  1 73 ... 37  1 73]\n",
      " [61 63 72 ... 70 75 74]\n",
      " ...\n",
      " [73  1 67 ...  1 56 69]\n",
      " [77 68  1 ... 74  1 56]\n",
      " [60 74 79 ... 76 59 66]], shape=(64, 50), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[74  7 73 ... 56 55 57]\n",
      " [ 1 73 55 ...  1 73 55]\n",
      " [63 72 66 ... 75 74 55]\n",
      " ...\n",
      " [ 1 67 59 ... 56 69 58]\n",
      " [68  1 74 ...  1 56 75]\n",
      " [74 79  1 ... 59 66  1]], shape=(64, 50), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(input_example_batch)\n",
    "    print(target_example_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3919,
     "status": "ok",
     "timestamp": 1574708264456,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "zCugZKN84Hsq",
    "outputId": "2a79096e-e61c-4f75-f7fe-59b6d0091f34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50, 96) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, each batch contains 64 samples, each sample has input and target which both constins 50 charaters. \n",
    "\n",
    "\n",
    "### Model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3903,
     "status": "ok",
     "timestamp": 1574708264458,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "Nymmt3PB4Hs5",
    "outputId": "9397c98f-aa45-4c46-a2d6-b4a29fe78f31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (64, None, 256)           24576     \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (64, None, 128)           148224    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (64, None, 128)           131584    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (64, None, 96)            12384     \n",
      "=================================================================\n",
      "Total params: 316,768\n",
      "Trainable params: 316,768\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample indices\n",
    "Input random lyrics and use untrained model to predict the lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZB1SSHgu4HtL"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3854,
     "status": "ok",
     "timestamp": 1574708264464,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "IwKSSQ724Hts",
    "outputId": "028058e8-1d85-49a4-f12e-1a8e037b90f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"me\\nWe could've been somebody\\nThought you'd be diff\"\n",
      "\n",
      "Next Char Predictions: \n",
      " '\\'”tFDpmMj\",9-·A7é5dUnJO8(O4r1Xa:A6*“zELC9 r2F’8U??'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and compile the model\n",
    "Catergorical crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3837,
     "status": "ok",
     "timestamp": 1574708264466,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "3gFfNnen4Ht-",
    "outputId": "fcda876b-bb38-4163-fd35-ad66b53167b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 50, 96)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.56478\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxrHj8Lr4HuM"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Here define a callback to save checkpoint for each epoch so that we can refer to the weight at each epoch at later step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpj3eGgK4Huo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpj3eGgK4Huo"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"c_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sTwlReiJ4HvI"
   },
   "outputs": [],
   "source": [
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2204718,
     "status": "ok",
     "timestamp": 1574714443040,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "Zg7rc4n54Hvl",
    "outputId": "b5574e20-5303-40bb-ef8d-fa17f5a2c8e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1209 05:37:12.945901  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer\n",
      "W1209 05:37:12.950873  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer.iter\n",
      "W1209 05:37:12.954874  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "W1209 05:37:12.957867  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "W1209 05:37:12.960873  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer.decay\n",
      "W1209 05:37:12.962870  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "W1209 05:37:12.967863  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "W1209 05:37:12.972859  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "W1209 05:37:12.974859  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "W1209 05:37:12.977857  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.forward_layer.cell.kernel\n",
      "W1209 05:37:12.979854  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.forward_layer.cell.recurrent_kernel\n",
      "W1209 05:37:12.982857  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.forward_layer.cell.bias\n",
      "W1209 05:37:12.985864  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.backward_layer.cell.kernel\n",
      "W1209 05:37:12.988850  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.backward_layer.cell.recurrent_kernel\n",
      "W1209 05:37:12.989853  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.backward_layer.cell.bias\n",
      "W1209 05:37:12.992847  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "W1209 05:37:12.994847  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "W1209 05:37:12.997844  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "W1209 05:37:13.009838  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.forward_layer.cell.kernel\n",
      "W1209 05:37:13.013835  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.forward_layer.cell.recurrent_kernel\n",
      "W1209 05:37:13.019833  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.forward_layer.cell.bias\n",
      "W1209 05:37:13.024834  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.backward_layer.cell.kernel\n",
      "W1209 05:37:13.027827  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.backward_layer.cell.recurrent_kernel\n",
      "W1209 05:37:13.030831  1292 util.py:144] Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.backward_layer.cell.bias\n",
      "W1209 05:37:13.037822  1292 util.py:152] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406/406 [==============================] - 331s 816ms/step - loss: 2.6772\n",
      "Epoch 2/10\n",
      "406/406 [==============================] - 385s 948ms/step - loss: 2.0154\n",
      "Epoch 3/10\n",
      "406/406 [==============================] - 425s 1s/step - loss: 1.8261\n",
      "Epoch 4/10\n",
      "406/406 [==============================] - 293s 723ms/step - loss: 1.7327\n",
      "Epoch 5/10\n",
      "406/406 [==============================] - 380s 936ms/step - loss: 1.6738\n",
      "Epoch 6/10\n",
      "406/406 [==============================] - 373s 919ms/step - loss: 1.6311\n",
      "Epoch 7/10\n",
      "406/406 [==============================] - 380s 936ms/step - loss: 1.5968\n",
      "Epoch 8/10\n",
      "271/406 [===================>..........] - ETA: 1:45 - loss: 1.5672"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loss=history.history['loss']\n",
    "epochs=range(len(loss)) # Get number of epochs\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Loss\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat a new model use trained weight\n",
    "\n",
    "Looking at the loss for each epoc, seems the 18th epoc works pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1574714443054,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "hm5jePC54Hvx",
    "outputId": "1eb24c77-9757-4124-b8e8-5450a51a4a82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\c_10'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQQUxgeG4Hv7"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "#model.load_weights('./training_checkpoints\\\\c_18')\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1574714443551,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "pYKWK-nb4HwJ",
    "outputId": "c0521733-268f-45fa-e489-4def611f7fde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (1, None, 256)            24576     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (1, None, 256)            296448    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (1, None, 96)             24672     \n",
      "=================================================================\n",
      "Total params: 345,696\n",
      "Trainable params: 345,696\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyrics Generator\n",
    "Finally define the lyrics generator function that feed in a string and predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3WbqXYtu4Hwb"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "    num_generate = 300\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char_dict[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "    temperature = 0.8\n",
    "\n",
    "  # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        #print(predictions)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        #print(tf.random.categorical(predictions, num_samples=1))\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "It is around Thanksgiving, let's try use \"Thanksgiving\" as start string and see what we will get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5999,
     "status": "ok",
     "timestamp": 1574714837975,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "k_BN2bzK4Hwl",
    "outputId": "eb184d15-c6cb-4748-ee83-99a5b892a4c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanksgiving\n",
      "Thinininininininininininininininininininininininininininininininininininininininininininininininsns s s s s m memememememememememe evevevevevevevevevevevevevevevevevevevevevevevevevevevevevenenenenenenenenenenenererererererererererererererererererererererererererererererer r l Wexexexexexexexexexex\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"Thanksgiving\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6iJX90G4Hwz"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Here I generated 1000 characters for given start string \"Thanksgiving\". Since the chance of line \"\\n\" and space are also been part of the training vocab, the auto-generated lyrics will switch lines between bars and skip a line between verses. Not looking at the contex, this lyrics looks somehow legit. Some intresting found after deep inspecting the lyrics:\n",
    "\n",
    "- The vocab is case senetive, so it's good to see the model generate the lyrics that captalize the first character of each line also some names like \"Coldplay\". \n",
    "\n",
    "- It also know to use quotation marks after the word \"say\", for example, They say Drive Slow, I say \"I know\".\n",
    "\n",
    "- Also, we can see the lyric is impacted by Kanye. For example, he is known as a Christian rapper. We can see some lyrics that are God related:\n",
    "    May the Lord forgive us, me it was this is Christians\n",
    "\n",
    "- Even rhyme can be seen sometime:\n",
    "\n",
    "Overall, the auto-generated lyrics of course is not as good as the original ones. However, it presents the ability of NLP and an possible way to generate text of different content. This model is trained using charater based RNN due to it has less unique \"labels\" so can largly reduce the computational expense. The drawback is the composition of sentances and verses may not make much sence to people. The model can be improved by using \"word-based\" vocab and input more samples and train.\n",
    "\n",
    "\n",
    "# Happy Thanksgiving!\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
