{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rap Lyrics Generator - Kanye West\n",
    "\n",
    "## Introduction\n",
    "This project uses 300+ Kanye West's verses to train on Recurrent Nural Network (RNN). The verses file is [from a dataset on Kaggle](https://www.kaggle.com/viccalexander/kanyewestverses). The dataset contains total of 364 verses from 243 songs. Each verses are seprated by an empty line which consist of \"\\n\". This project is trained on characters based features. i.e. the embeding step is based on characters, not words.\n",
    "This project is inspried by a NLP course on coursera in Tensorflow in practice specitialization offer by deeplearning.ai and [Tensorflow tutorial](https://www.tensorflow.org/tutorials/text/text_generation). The process is to prepare the lyrics and window them into segments the use the characters in these windows to predict the next character. Since each character is related to its previous and next character, RNN and LSTM that carry hidden state from previous cell are suitable for this application. \n",
    "\n",
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2840,
     "status": "ok",
     "timestamp": 1574708262548,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "p-BI8gkj4Hp1",
    "outputId": "41d1274b-cd51-44ee-c4af-d81bd53a67c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "#!pip install tensorflow==2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the file print the first 250 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2770,
     "status": "ok",
     "timestamp": 1574708262552,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "RgeWVKN94HqN",
    "outputId": "4eab332c-1b01-4e9b-bb3f-d9ea7befc86b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 260341 characters\n",
      "Let the suicide doors up\n",
      "I threw suicides on the tour bus\n",
      "I threw suicides on the private jet\n",
      "You know what that mean, I'm fly to death\n",
      "I step in Def Jam buildin' like I'm the shit\n",
      "Tell 'em give me fifty million or I'ma quit\n",
      "Most rappers' taste level\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open('kanye_verses.txt', 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "\n",
    "print ('Length of text: {} characters'.format(len(text)))\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique character\n",
    "We are using character based feature, so we want to identify the total number of unique charactor and give each one of them a numerical label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2670,
     "status": "ok",
     "timestamp": 1574708262555,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "8HzRqvHK4Hqd",
    "outputId": "fd5d512a-3cc1-48b5-d5b5-e421b25f0fc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 unique characters:\n",
      "['\\n', ' ', '!', '\"', '#', '$', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~', '·', 'Á', 'é', 'í', 'ñ', 'ó', 'ā', '\\u200b', '–', '‘', '’', '“', '”', '…']\n",
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '#': 4, '$': 5, '&': 6, \"'\": 7, '(': 8, ')': 9, '*': 10, '+': 11, ',': 12, '-': 13, '.': 14, '/': 15, '0': 16, '1': 17, '2': 18, '3': 19, '4': 20, '5': 21, '6': 22, '7': 23, '8': 24, '9': 25, ':': 26, ';': 27, '?': 28, 'A': 29, 'B': 30, 'C': 31, 'D': 32, 'E': 33, 'F': 34, 'G': 35, 'H': 36, 'I': 37, 'J': 38, 'K': 39, 'L': 40, 'M': 41, 'N': 42, 'O': 43, 'P': 44, 'Q': 45, 'R': 46, 'S': 47, 'T': 48, 'U': 49, 'V': 50, 'W': 51, 'X': 52, 'Y': 53, 'Z': 54, 'a': 55, 'b': 56, 'c': 57, 'd': 58, 'e': 59, 'f': 60, 'g': 61, 'h': 62, 'i': 63, 'j': 64, 'k': 65, 'l': 66, 'm': 67, 'n': 68, 'o': 69, 'p': 70, 'q': 71, 'r': 72, 's': 73, 't': 74, 'u': 75, 'v': 76, 'w': 77, 'x': 78, 'y': 79, 'z': 80, '~': 81, '·': 82, 'Á': 83, 'é': 84, 'í': 85, 'ñ': 86, 'ó': 87, 'ā': 88, '\\u200b': 89, '–': 90, '‘': 91, '’': 92, '“': 93, '”': 94, '…': 95}\n"
     ]
    }
   ],
   "source": [
    "# number of unique characters \n",
    "print(len(set(text)),'unique characters:') \n",
    "\n",
    "# dict of these characters\n",
    "chars = sorted(set(text))\n",
    "print(chars)\n",
    "\n",
    "char_dict = {char:i for i,char in enumerate(chars)}\n",
    "idx2char = np.array(chars)\n",
    "print(char_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform lyrics to numerical labels\n",
    "Here each characters are transform to relative numerical label that I created above so they can be then feed into Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2552,
     "status": "ok",
     "timestamp": 1574708262558,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "V78sCket4Hqr",
    "outputId": "f2b83a6f-b0fb-4670-9f3f-937a11d8b0de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let the suici\n",
      "[40 59 74  1 74 62 59  1 73 75 63 57 63]\n",
      "'Let the suici' ---- characters mapped to int ---- > [40 59 74  1 74 62 59  1 73 75 63 57 63]\n"
     ]
    }
   ],
   "source": [
    "#Sequence of the text\n",
    "text_in_num =np.array([char_dict[i] for i in text])\n",
    "print(text[:13])\n",
    "print(text_in_num[:13])\n",
    "\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_in_num[:13]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequences for training X and Y\n",
    "In order to create X (feature set) and Y, the answer,(ground truth), the entire lyrics file are flaten to a single vactor(1,# of chars). The sequences (training samples) are extracted by shifting a window down to the vector. For example, If the lyrics file in a single vector is : \\[0,1,2,3,4,5,6,7,8,9\\]. We use a window size of 3 and shift every 1 character, we will get:\\[0,1,2\\] \\[1,2,3\\] \\[2,3,4\\] \\[3,4,5\\] \\[4,5,6\\] \\[5,6,7\\] \\[6,7,8\\] and \\[7,8,9\\].\n",
    "\n",
    "The window size is defined by the length of each sequence(training sample). The sample is the first character to n-1 th. The second to the last characters are our target, Y. i.e. an instance is \\[0,1,2,3\\]: Training sequence :\\[0,1,2\\], target:\\[1,2,3\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2650,
     "status": "ok",
     "timestamp": 1574708262748,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "J35F4NZx4Hq2",
    "outputId": "3b8869d7-d3a9-47db-d156-7c5935cc9da7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52058"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert to trainable data\n",
    "seq_len = 50\n",
    "example_per_epoc = (len(text_in_num)-seq_len)//5\n",
    "BATCH_SIZE = 64\n",
    "example_per_epoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2596,
     "status": "ok",
     "timestamp": 1574708262751,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "JPSeMcEG4HrI",
    "outputId": "1da38800-f716-4f85-a76c-929f327d2e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:(batch_size, sequence_len)\n",
      "(64, 50)\n",
      "Target:(batch_size,)\n",
      "(64, 50)\n"
     ]
    }
   ],
   "source": [
    "# Create training examples / targets\n",
    "\n",
    "#Make the data in numerical lable to Tf.dataset for furter processing\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_in_num)\n",
    "\n",
    "#Creating sequences \n",
    "sequences = char_dataset.window(size = seq_len+1, shift = 10, drop_remainder = True)\n",
    "\n",
    "#Get training instance (training sequence + Target) and Flaten out the entire dataset \n",
    "sequences = sequences.flat_map(lambda window: window.batch(seq_len+1))\n",
    "\n",
    "#Creating tuple for each train instance : (sequence, target)\n",
    "dataset = sequences.map(lambda window: (window[:-1], window[1:]))\n",
    "\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "\n",
    "for i,y in dataset.take(1):\n",
    "    print('Sample:(batch_size, sequence_len)')\n",
    "    print(i.numpy().shape)\n",
    "    print('Target:(batch_size,)')\n",
    "    print(y.numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the first 10 sequences with window size 50 and shift = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2559,
     "status": "ok",
     "timestamp": 1574708262754,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "sGAW7OOL4HrT",
    "outputId": "bcc4177f-ce60-4ad8-b1cb-f64ef0be27df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"p, that pussy slippery, no whip\\nWe ain't trippin' \"\n",
      "Input data:  \" problem is I be textin'\\nMy psychiatrist got kids \"\n",
      "Input data:  'Huh? Motherfucker we rolling\\nWith some light-skinn'\n",
      "Input data:  ' hoodrats\\nAnd I just blame everything on you\\nAt le'\n",
      "Input data:  'o see Thee more clearly\\nI know he hear me when my '\n",
      "Input data:  \" a whole lot of O's\\nWhat you after, actor money?\\nY\"\n",
      "Input data:  \"n this bitch another 'gain\\n\\nI made Jesus Walks, I'\"\n",
      "Input data:  \"ike Mekhi Phife'\\nIn that pussy so deep I could hav\"\n",
      "Input data:  'at we been through\\nI mean, after all the things we'\n",
      "Input data:  \"lla\\nOkay, I smashed your Corolla\\nI'm hangin' on a \"\n"
     ]
    }
   ],
   "source": [
    "for batch,batch_target in  dataset.take(1):\n",
    "    for i in range(10):\n",
    "        print ('Input data: ', repr(''.join(idx2char[batch[i].numpy()])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjizZxG44Hre"
   },
   "source": [
    "### The first 5 sequences and respective targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2466,
     "status": "ok",
     "timestamp": 1574708262913,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "ynOc_FYQ4Hrm",
    "outputId": "1d29b4ed-c6f8-466a-b22c-dd1885aac412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \", everybody gettin' paid\\nNiggas lookin' at me like\"\n",
      "Target data: \" everybody gettin' paid\\nNiggas lookin' at me like \"\n",
      "Input data:  \" like a month right now\\nStupid niggas gettin' mone\"\n",
      "Target data: \"like a month right now\\nStupid niggas gettin' money\"\n",
      "Input data:  \"ing I need to let you know\\nYou ain't never seen no\"\n",
      "Target data: \"ng I need to let you know\\nYou ain't never seen not\"\n",
      "Input data:  'very ass, cheated on every test\\nI guess, this is m'\n",
      "Target data: 'ery ass, cheated on every test\\nI guess, this is my'\n",
      "Input data:  \" other side\\nGotta keep 'em separated, I call that \"\n",
      "Target data: \"other side\\nGotta keep 'em separated, I call that a\"\n"
     ]
    }
   ],
   "source": [
    "for batch,batch_target in  dataset.take(1):\n",
    "    for i in range(5):\n",
    "        print ('Input data: ', repr(''.join(idx2char[batch[i].numpy()])))\n",
    "        print ('Target data:', repr(''.join(idx2char[batch_target[i].numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2432,
     "status": "ok",
     "timestamp": 1574708262917,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "rVZsMEEd4Hr9",
    "outputId": "a8e8b2fe-df84-4e54-c7fd-763b889b8bd7"
   },
   "source": [
    "### Build the model\n",
    "The model consists of an embeding layer, a RNN (GRU) layer, and a dense output layer. As the \"return_sequence\" is set to True, the output of GRU layer is output the state of each call in the GRU. The dense lyaer outputs the probability of each charater for each state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fy4n3BOo4HsJ"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dTY2zdCW4HsY"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    # Emdedding layer    \n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "        \n",
    "    # RNN layer    \n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "        \n",
    "    # Dense layer    \n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "      ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "  vocab_size = len(chars),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look what each batch looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[70 75 74 ... 57 65  1]\n",
      " [62 63  0 ... 59  1 70]\n",
      " [59 77 59 ... 55 74 69]\n",
      " ...\n",
      " [79 59 72 ...  1 77 59]\n",
      " [69 75 68 ... 74  0 48]\n",
      " [66 58  1 ... 74 57 62]], shape=(64, 50), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[75 74  1 ... 65  1 59]\n",
      " [63  0 47 ...  1 70 55]\n",
      " [77 59 66 ... 74 69 73]\n",
      " ...\n",
      " [59 72 73 ... 77 59  1]\n",
      " [75 68 58 ...  0 48 62]\n",
      " [58  1 58 ... 57 62  1]], shape=(64, 50), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(input_example_batch)\n",
    "    print(target_example_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3919,
     "status": "ok",
     "timestamp": 1574708264456,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "zCugZKN84Hsq",
    "outputId": "2a79096e-e61c-4f75-f7fe-59b6d0091f34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50, 96) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, each batch contains 64 samples, each sample has input and target which both constins 50 charaters. \n",
    "\n",
    "\n",
    "### Model summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3903,
     "status": "ok",
     "timestamp": 1574708264458,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "Nymmt3PB4Hs5",
    "outputId": "9397c98f-aa45-4c46-a2d6-b4a29fe78f31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           24576     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 96)            98400     \n",
      "=================================================================\n",
      "Total params: 4,061,280\n",
      "Trainable params: 4,061,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample indices\n",
    "Input random lyrics and use untrained model to predict the lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZB1SSHgu4HtL"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3854,
     "status": "ok",
     "timestamp": 1574708264464,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "IwKSSQ724Hts",
    "outputId": "028058e8-1d85-49a4-f12e-1a8e037b90f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'ay\\nSo this is in the name of love like Robert say\\n'\n",
      "\n",
      "Next Char Predictions: \n",
      " \"Lq6:·Gjio‘'qP\\nuqiCw&uāTa5LYT+EcJkjCh\\u200b”P)F·~ó;PT+e”\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and compile the model\n",
    "Catergorical crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3837,
     "status": "ok",
     "timestamp": 1574708264466,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "3gFfNnen4Ht-",
    "outputId": "fcda876b-bb38-4163-fd35-ad66b53167b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 50, 96)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.562809\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxrHj8Lr4HuM"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Here define a callback to save checkpoint for each epoch so that we can refer to the weight at each epoch at later step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpj3eGgK4Huo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpj3eGgK4Huo"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"c_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sTwlReiJ4HvI"
   },
   "outputs": [],
   "source": [
    "EPOCHS=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2204718,
     "status": "ok",
     "timestamp": 1574714443040,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "Zg7rc4n54Hvl",
    "outputId": "b5574e20-5303-40bb-ef8d-fa17f5a2c8e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "406/406 [==============================] - 794s 2s/step - loss: 2.3300\n",
      "Epoch 2/30\n",
      "406/406 [==============================] - 799s 2s/step - loss: 1.6749\n",
      "Epoch 3/30\n",
      "406/406 [==============================] - 802s 2s/step - loss: 1.4074\n",
      "Epoch 4/30\n",
      "406/406 [==============================] - 803s 2s/step - loss: 1.1648\n",
      "Epoch 5/30\n",
      "406/406 [==============================] - 805s 2s/step - loss: 0.9285\n",
      "Epoch 6/30\n",
      "406/406 [==============================] - 801s 2s/step - loss: 0.7483\n",
      "Epoch 7/30\n",
      "406/406 [==============================] - 796s 2s/step - loss: 0.6407\n",
      "Epoch 8/30\n",
      "406/406 [==============================] - 832s 2s/step - loss: 0.5821\n",
      "Epoch 9/30\n",
      "406/406 [==============================] - 830s 2s/step - loss: 0.5502\n",
      "Epoch 10/30\n",
      "406/406 [==============================] - 814s 2s/step - loss: 0.5323\n",
      "Epoch 11/30\n",
      "406/406 [==============================] - 814s 2s/step - loss: 0.5191\n",
      "Epoch 12/30\n",
      "406/406 [==============================] - 807s 2s/step - loss: 0.5101\n",
      "Epoch 13/30\n",
      "406/406 [==============================] - 818s 2s/step - loss: 0.5055\n",
      "Epoch 14/30\n",
      "406/406 [==============================] - 795s 2s/step - loss: 0.5011\n",
      "Epoch 15/30\n",
      "406/406 [==============================] - 797s 2s/step - loss: 0.4964\n",
      "Epoch 16/30\n",
      "406/406 [==============================] - 788s 2s/step - loss: 0.4944\n",
      "Epoch 17/30\n",
      "406/406 [==============================] - 807s 2s/step - loss: 0.4948\n",
      "Epoch 18/30\n",
      "406/406 [==============================] - 793s 2s/step - loss: 0.4918\n",
      "Epoch 19/30\n",
      "406/406 [==============================] - 784s 2s/step - loss: 0.4925\n",
      "Epoch 20/30\n",
      "406/406 [==============================] - 790s 2s/step - loss: 0.4950\n",
      "Epoch 21/30\n",
      "406/406 [==============================] - 807s 2s/step - loss: 0.4965\n",
      "Epoch 22/30\n",
      "406/406 [==============================] - 793s 2s/step - loss: 0.4981\n",
      "Epoch 23/30\n",
      "406/406 [==============================] - 796s 2s/step - loss: 0.4993\n",
      "Epoch 24/30\n",
      "406/406 [==============================] - 793s 2s/step - loss: 0.5025\n",
      "Epoch 25/30\n",
      "406/406 [==============================] - 795s 2s/step - loss: 0.5058\n",
      "Epoch 26/30\n",
      "406/406 [==============================] - 795s 2s/step - loss: 0.5098\n",
      "Epoch 27/30\n",
      "406/406 [==============================] - 791s 2s/step - loss: 0.5190\n",
      "Epoch 28/30\n",
      "406/406 [==============================] - 798s 2s/step - loss: 0.5253\n",
      "Epoch 29/30\n",
      "406/406 [==============================] - 792s 2s/step - loss: 0.5382\n",
      "Epoch 30/30\n",
      "406/406 [==============================] - 785s 2s/step - loss: 0.5502\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat a new model use trained weight\n",
    "\n",
    "Looking at the loss for each epoc, seems the 18th epoc works pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1574714443054,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "hm5jePC54Hvx",
    "outputId": "1eb24c77-9757-4124-b8e8-5450a51a4a82"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\c_30'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQQUxgeG4Hv7"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "#model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.load_weights('./training_checkpoints\\\\c_18')\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1574714443551,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "pYKWK-nb4HwJ",
    "outputId": "c0521733-268f-45fa-e489-4def611f7fde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (1, None, 256)            24576     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 96)             98400     \n",
      "=================================================================\n",
      "Total params: 4,061,280\n",
      "Trainable params: 4,061,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lyrics Generator\n",
    "Finally define the lyrics generator function that feed in a string and predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3WbqXYtu4Hwb"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char_dict[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "    temperature = 0.65\n",
    "\n",
    "  # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        #print(predictions)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        #print(tf.random.categorical(predictions, num_samples=1))\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "It is around Thanksgiving, let's try use \"Thanksgiving\" as start string and see what we will get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5999,
     "status": "ok",
     "timestamp": 1574714837975,
     "user": {
      "displayName": "Fred Yen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mB6nxbYGKaquw4vPrPGHaR1suwnJn5ef6C6H-HZBw=s64",
      "userId": "05465226488795521239"
     },
     "user_tz": 360
    },
    "id": "k_BN2bzK4Hwl",
    "outputId": "eb184d15-c6cb-4748-ee83-99a5b892a4c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanksgiving, do we even got a question?\n",
      "Hermes pastel, I say \"I know he got a plan, I know I'm on your beams\n",
      "One set of footsteps, you was carryin' me\n",
      "When I was pretty before the dough but now I'm just the man\n",
      "But she not like you\n",
      "Right now I need you to muth of me\n",
      "I will not be headed astronaut\n",
      "Maybe it's because\n",
      "That girl is bad\n",
      "\n",
      "I'm getting spins a song with Coldplay\n",
      "Back in my mind I'm like a vampire on the gas fully filled up like I nigga\n",
      "\n",
      "May the Lord forgive us, me it was this is Christians\n",
      "My apologies, they be all up on your ass, yo\n",
      "When you gettin' money, cops don’t fall\n",
      "Made to make niggas fail\n",
      "Especially if you paid for the way that she 36-2, pieces catch miracle whips\n",
      "Changing lanes\n",
      "Yeah, I'm changing lanes\n",
      "\n",
      "Magazines call me a rock star, girls call me cock star\n",
      "Billboard, pop star, neighborhood black people to the one\n",
      "I know they don't want niggas that life\n",
      "Wele as the light post\n",
      "They say Drive Slow, I say \"I know\"\n",
      "Then errrr, away I go\n",
      "And the way I want you\n",
      "I'm goin to tell you w\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"Thanksgiving\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6iJX90G4Hwz"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Here I generated 1000 characters for given start string \"Thanksgiving\". Since the chance of line \"\\n\" and space are also been part of the training vocab, the auto-generated lyrics will switch lines between bars and skip a line between verses. Not looking at the contex, this lyrics looks somehow legit. Some intresting found after deep inspecting the lyrics:\n",
    "\n",
    "- The vocab is case senetive, so it's good to see the model generate the lyrics that captalize the first character of each line also some names like \"Coldplay\". \n",
    "\n",
    "- It also know to use quotation marks after the word \"say\", for example, They say Drive Slow, I say \"I know\".\n",
    "\n",
    "- Also, we can see the lyric is impacted by Kanye. For example, he is known as a Christian rapper. We can see some lyrics that are God related:\n",
    "    May the Lord forgive us, me it was this is Christians\n",
    "\n",
    "- Even rhyme can be seen sometime:\n",
    "\n",
    "Overall, the auto-generated lyrics of course is not as good as the original ones. However, it presents the ability of NLP and an possible way to generate text of different content. This model is trained using charater based RNN due to it has less unique \"labels\" so can largly reduce the computational expense. The drawback is the composition of sentances and verses may not make much sence to people. The model can be improved by using \"word-based\" vocab and input more samples and train.\n",
    "\n",
    "\n",
    "# Happy Thanksgiving!\n",
    "    \n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
